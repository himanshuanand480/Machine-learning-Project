# -*- coding: utf-8 -*-
"""Heart_desease_prediction_self.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10GRl09IuozFqEnDmYc-780E1UlCMtQuQ
"""

import seaborn as sns
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

df=pd.read_csv('/content/heart (1).csv')
df

df.head()

"""**Take random 5 data so we can know the how the data is follow the trained**"""

df.sample(5)

df.info()

df.describe()#describe use to get the statical dataset

"""**tail is use to get the data from the last 5 data from the dataset**"""

df.tail()

df.describe(include="all")#include="all" is use to get the all column irrespective of datatype.

"""**Data Preprocessing**

**a) isnull->To get the column contain null values or not**
"""

df.isnull()

"""**b)isnull().sum--> use to get the sum of the all data column that contain null value**"""

df.isnull().sum()

"""**c)duplicate()-> use to get the column which contain the duplicate value**"""

df.duplicated()

"""**d)duplicatted().sum--> use to get the the sum of all the data which containg dupicate value**"""

df.duplicated().sum()

"""**e)To check the number of unique values in the dataset we use-> nuique()**"""

df.nunique()

"""**f)columns--> use to get the all the column name so we can able to determine which column is categorical or which is numerical data type.**"""

df.columns

"""**e)(include="obect").columns -> use to get all the columns which contains the object data data types**"""

cat_data=df.select_dtypes(include="object").columns
cat_data

"""**f)unique()->use to get how many different variety of data typpe's name in inside the data

* Lets access one by one
"""

df['ChestPainType'].unique()

df['RestingECG'].unique()

df['Sex'].unique()

df['ST_Slope'].unique()

df['ExerciseAngina'].unique()

"""**g)convert categorical data type to Numeric**

*   Sex: M=0,F=1
*   ChestPainType: ATA=0,NAP=1,ASY=2,TA=3
*   RestingECG:Normal=0,ST=1,LVH=2
*   ExerciseAngina:N=0,Y=1
*   ST_Slope:Up=0,Flat=1,Down=2
NOTE:- *You can able to choose 0,1 anyone it is depend upon your choice.*






"""

for i in cat_data:
  print(i)
  print(df[i].unique(),list(range(df[i].nunique())))
  df[i]=df[i].map(dict(zip(df[i].unique(),list(range(df[i].nunique())))))
  #df[i]=df[i].astype('int64')
  print('--'*40)
  print()

"""**h)dtype-->use to get the data type that our data type is float,int,bool etc in nature**"""

df['ST_Slope'].dtype

"""
*   see the how dataset is looking like after the Data Preprocessing

"""

df

"""**i).value_counts()-->use to count the data type's length**

*   Lets see one by one so it is more clear



"""

df['ChestPainType'].value_counts()

df['RestingECG'].value_counts()

df['Sex'].value_counts()

df['Cholesterol'].value_counts()

df['ExerciseAngina'].value_counts()

np.NaN

df['Cholesterol'].replace(0,np.NaN,inplace=True)

from sklearn.impute import KNNImputer

df.isnull().sum()

"""**Value of Cholesterol is Never 0 so we replace the NaN value**"""

imputer=KNNImputer(n_neighbors=3)
a_impute=imputer.fit_transform(df)
df=pd.DataFrame(a_impute,columns=df.columns)

df['Cholesterol'].isna().sum()

df['Cholesterol'].value_counts()

count0=0
for i in df['Cholesterol']:
  if i==0:
    count0+=1
print(count0)

"""****

*   *so there is no 0 value for cholesterol*

"""

df['RestingBP'].replace(0,np.nan,inplace=True)

"""**Similarly do for Resting Blood Pressure because values of Blood Pressure is never be zero**"""

df['RestingBP'].isna().sum()

from sklearn.impute import KNNImputer
df['RestingBP'].replace(0,np.nan,inplace=True)
imputer=KNNImputer(n_neighbors=3)
a_impute=imputer.fit_transform(df)
df=pd.DataFrame(a_impute,columns=df.columns)

"""

*   Lets see the how many null value is still present inside RestingBP

"""

df['RestingBP'].isna().sum()

"""****"""

df

df['RestingBP'].dtype

df['RestingBP'].value_counts()

"""****

1.   Lets see which row of rstingBP column is float or integer


"""

# Convert 'RestingBP' to numeric, coerce errors to NaN
df['RestingBP'] = pd.to_numeric(df['RestingBP'], errors='coerce')

# Create a boolean mask for float values (excluding NaN)
float_mask = df['RestingBP'].apply(lambda x: isinstance(x, float) and not pd.isna(x))

# Get the rows with float values
float_rows = df[float_mask]

# Display the rows with float values in 'RestingBP'
print(float_rows)

"""****

2.so we see there is no any row of RestingBP column which contains the meaning Float

3. so we convert them into integer




"""



"""**Data visualization**"""

df.corr()

"""

*   We see the correlation between the all the dataset
*   Now see the correlation between the HeartDisease column and other columns



"""

df.corr()['HeartDisease']

"""
*   Lets see the correlation HeartDisease column and other columns by sorted values so we can easily able to got it.



"""

df.corr()['HeartDisease'].sort_values()

px.line(df.corr()['HeartDisease'].sort_values(),x=df.corr()['HeartDisease'].sort_values().index)

"""
*   Lets see the correlation between the HeartDisease and other columns in form of graph for more clear visualization

"""

px.line(df.corr()['HeartDisease'][::-1].sort_values())

"""**bold text**"""

plt.plot(df["HeartDisease"],df['Age'],"o")
plt.xlabel("HeartDisease")
plt.ylabel("Age")
plt.title("Relationship between HeartDisease and Age")
plt.show()  # Display the plot

sns.pairplot(df)

sns.pairplot(df,hue="HeartDisease")

sns.pairplot(df,hue="Sex")

sns.pairplot(df,hue="ChestPainType")
plt.show()

sns.lmplt(x="Age",y="HeartDisease",data=df)

plt.hist(df['Age'])

px.histogram(df['HeartDisease'])

sns.jointplot(x="Age",y="HeartDisease",data=df)

corr=df['Age'].corr(df['HeartDisease'])
corr

px.histogram(df,x="Age",color="HeartDisease")

"""As we see Heart disease is increases with age,
Lets see how heart diseasew is related with sex
"""

px.histogram(df,x="Sex",color="HeartDisease")

df

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(
    df.drop('HeartDisease',axis=1),#drop this column because this is are our main column
    df['HeartDisease'],
    test_size=0.2,
    random_state=42,
    stratify=df['HeartDisease']
)

"""*  so the given data set is of Logistiuc Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
solve=['lbfgs','liblinear','newton-cg','sag','saga']
best_solv=''
test_score=np.zeros(6)
for i,n in enumerate(solve):
  model=LogisticRegression(solver=n).fit(x_train,y_train)
  test_score[i]=model.score(x_test,y_test)
  if model.score(x_test,y_test)==test_score.max():
    best_solv=n
model=LogisticRegression(solver=best_solv)
model.fit(x_train,y_train)
model_pred=model.predict(x_test)
print("LogisticRegression score of accuracy:",accuracy_score(y_test,model_pred))

import pickle
file=open("Logistic Regression.pkl","wb")
pickle.dump(model_pred,file)

"""**Support Vector Machine(SVM)**"""

from sklearn.svm import SVC
from sklearn.metrics import f1_score



"""**Decision Tree Maker**"""

kernel={'linear':0,'poly':0,'rbf':0,'sigmoid':0}
best_kernel=''
test_score=np.zeros(4)
for i in kernel:
  svm=SVC(kernel=i)
  svm.fit(x_train,y_train)
  y_hat=svm.predict(x_test)
  kernel[i]=f1_score(y_test,y_hat,average="weighted")
  if kernel[i]==max(kernel.values()):
    best_kernel=i
print(best_kernel)
svm=SVC(kernel=best_kernel)
svm.fit(x_train,y_train)
svm_pred=svm.predict(x_test)
print("SVM f1_score kernel",best_kernel,f1_score(y_test,svm_pred,average='weighted'))

import pickle
file=open("Logistic Regression.pkl","wb")
pickle.dump(Ctree,file)

import pickle
from google.colab import files

# Assuming 'svm' is your trained SVM model
with open("svm.pkl", "wb") as file:  # Save the model first
    pickle.dump(svm, file)

files.download("svm.pkl") #Now download it

import pickle
file=open("Decision Tree.pkl","wb")
pickle.dump(svm_pred,file)

import pickle
file=open("Logistic Regression.pkl","wb")
pickle.dump(Ctree,file)

import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris  # Replace with your heart disease dataset

# Load dataset (Replace with actual dataset)
data = load_iris()
X, y = data.data, data.target

# Split into train-test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# Save models
with open("RandomForest.pkl", "wb") as file:
    pickle.dump(rf_model, file)

with open("LogisticRegression.pkl", "wb") as file:
    pickle.dump(lr_model, file)

print("Models saved successfully!")

import pickle
from google.colab import files

# Assuming 'svm' is your trained SVM model
with open("tree.pkl", "wb") as file:  # Save the model first
    pickle.dump(svm, file)

files.download("tree.pkl") #Now download it

import pickle
from google.colab import files

# Assuming 'svm' is your trained SVM model
with open("gridrf.pkl", "wb") as file:  # Save the model first
    pickle.dump(svm, file)

files.download("gridrf.pkl") #Now download it

import pickle
file=open("RandomForest.pkl","wb")
pickle.dump(rf_model,file)